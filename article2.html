<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <meta content="description" name="description">
    <meta name="google" content="notranslate"/>
    <meta content="Mashup templates have been developped by Orson.io team" name="author">

    <!-- Disable tap highlight on IE -->
    <meta name="msapplication-tap-highlight" content="no">

    <link rel="robot-icon" sizes="180x180" href="./assets/robot-icon-180x180.png">
    <link href="./assets/robot-icon-180x180.ico" rel="icon">

    <title>RL - VeilleTech</title>

    <link href="./main.3f6952e4.css" rel="stylesheet">
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
    <nav class="navbar  navbar-fixed-top navbar-default">
        <div class="container">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbar-collapse">
                 <ul class="nav navbar-nav ">
                    <li><a href="./index.html" title="">Home</a></li>
                    <li><a href="./intro.html" title="">Introduction</a></li>
                    <li><a href="./article1.html" title="">Bases théoriques du RL</a></li>
                    <li><a href="./article2.html" title="">Applications du RL</a></li>
                    <li><a href="./article3.html" title="">RL & Finance</a></li>
                    <li><a href="./news.html" title="">Ressources</a></li>
                </ul>


            </div>
        </div>
    </nav>
</header>
    
    
<div class="section-container">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
                <div class="card-container">
                    <div class="text-center">
                        <h1 class="h2">Applications of RL in Real World</h1>
                    </div>
                    <p style="text-align:center; font-size:120%;">Cet article présente met la lumière sur diverses applications de RL 
                        dans l'industrie </p>

                </div>
            </div>


            <div class="col-md-8 col-md-offset-2 section-container-spacer">
                <div class="row">
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/gaming_rl.png" class="img-responsive" alt="">
                        <p>Reinforcement Learning & Gaming</p>
                    </div>
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/robotics_rl.jpeg" class="img-responsive" alt="">
                        <p>Reinforcement Learning & Robotics</p>
                    </div>
                </div>
            </div>

            <div class="col-xs-12">

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Alors que les techniques de Deep Learning deviennent de plus en plus utilisées en industrie, notamment
                    à cause de leurs applications en vision et en NLP, le Reinforcement Learning semble être pour le moment 
                    sous-évalué.<br>
                    En outre, il semble qu'il y ait peu de ressources pour expliquer en détail comment le RL est appliquée dans
                    différentes industries. <br>
                    Ceci dit, l'apprentissage par renforcement ne devrait pas être négligé dans la
                    recherche en entreprise, étant donné son énorme potentiel d'aide à la prise de décision. 
                    Comme l'a dit Koray Kavukcuoglu, directeur de la recherche chez Deepmind :
                </p>
                    
                    <blockquote>
                        <p>“Combination of DL and RL is the best answer we have so far in terms of learning very good state
                            representations of challenging tasks to solve challenging real world problems.”</p>
                        <small class="pull-right">Koray Kavukcuoglu</small>
                    </blockquote>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Par conséquent, le présent article vise à examiner l'étendue des applications du RL
                    dans le monde réel à l'heure actuelle, le lien du RL à d'autres domaines ainsi que le potentiel 
                    d'utilisation du RL dans le futur.
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>

                
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Gestion de feux de signalisation</b>
                </p>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Dans l'article "Renforcement d'un système multi-agents basé sur l'apprentissage pour le contrôle des feux de signalisation sur réseau"[3], 
                    les chercheurs ont essayé de concevoir un contrôleur de feux de signalisation pour résoudre le problème de congestion. 
                    Cependant, testées uniquement sur un environnement simulé, leurs méthodes ont donné des résultats 
                    supérieurs à ceux des méthodes traditionnelles et ont mis en lumière les utilisations potentielles 
                    de la NR multi-agent dans la conception des systèmes de circulation.
                    
                    Cinq agents ont été placés dans le réseau de circulation à cinq intersections, avec un agent de RL 
                    à l'intersection centrale pour contrôler la signalisation routière. L'état a été défini comme un vecteur à
                    huit dimensions, chaque élément représentant le flux de trafic relatif de chaque voie. Huit choix s'offraient 
                    à l'agent, chacun représentant une combinaison de phases, et la fonction de récompense était définie comme 
                    la réduction du délai par rapport au pas de temps précédent. 
                </p>
                
                <p align="center">
                    <img src="./assets/images/autoencoder.jpg" class="img-responsive" alt="" align="middle">
                </p>

                
                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Robotique</b>
                </p>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Il y a un travail énorme sur l'application de la RL en robotique. En particulier,[11] a formé un robot 
                    pour qu'il apprenne les politiques de mise en correspondance des images vidéo brutes avec les actions du robot.
                    Les images RVB ont été transmises à un CNN et les sorties étaient les couples moteur. 
                    L'élément RL était la recherche politique guidée pour générer des données sur la formation qui provenaient 
                    de sa propre répartition par État.
                </p>
                
                <p align="center">
                    <iframe width="784" height="441" src="https://www.youtube.com/watch?v=Q4bMcUk6pcw"
                            frameborder="0" allow="accelerometer;encrypted-media; gyroscope; picture-in-picture" allowfullscreen>

                    </iframe>
                </p>

                
                
                <div style="height:20px;font-size:20px;">&nbsp;</div
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Gaming</b>
                </p>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    RL est si bien connu de nos jours parce que c'est l'algorithme principal utilisé pour résoudre différents jeux
                    et parfois atteindre des performances surhumaines.<br>
                    Le plus célèbre doit être AlphaGo[12] et AlphaGo Zero[13]. AlphaGo, formé à d'innombrables jeux humains, 
                    a déjà atteint une performance surhumaine en utilisant le réseau de valeurs et la recherche par arbre 
                    de Monte Carlo (SCTM) dans son réseau de politiques. Pourtant, les chercheurs ont réfléchi plus tard et
                    ont essayé une approche plus pure de la LRR - l'entraîner à partir de zéro. Les chercheurs ont laissé 
                    le nouvel agent, AlphaGo Zero, jouer avec lui-même et finalement battre AlphaGo 100-0.
                </p>

                <p align="center">
                    <img width="784" height="441" src="./assets/images/latentspace.jpg" class="img-responsive" alt="" align="middle">
                </p>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    RL a une relation très étroite avec la psychologie, la biologie et les neurosciences. Si vous y réfléchissez bien, 
                    ce que fait un agent de NR n'est qu'un essai-erreur : il apprend à quel point ses actions sont bonnes ou mauvaises
                    en fonction des récompenses qu'il reçoit de l'environnement. Et c'est exactement comme ça que l'homme apprend 
                    à prendre une décision. Par ailleurs, le problème de l'exploration et de l'exploitation, le problème de la 
                    cession de crédit, les tentatives de modélisation de l'environnement sont aussi des problèmes auxquels 
                    nous sommes confrontés dans notre vie quotidienne.
                    
                    La théorie de l'économie peut aussi éclairer un peu la question de la RL. En particulier, 
                    l'analyse de l'apprentissage du renforcement multi-agent (MARL) peut être comprise à partir des perspectives 
                    de la théorie des jeux, qui est un domaine de recherche développé par John Nash pour comprendre les interactions 
                    des agents dans un système. En plus de la théorie des jeux, le MARL, Partially Observable Markov Decision Process (POMDP) pourrait également être utile pour comprendre d'autres sujets économiques comme la structure du marché (monopole, oligopole, etc.), l'externalité et l'asymétrie d'information.
                </p>

                <div style="height:30px;font-size:20px;">&nbsp;</div>

                <p align="center">
                    <img src="./assets/images/semantic-map.gif" class="img-responsive" alt="" align="middle">
                </p>
                <p></p>

                <div style="height:30px;font-size:20px;">&nbsp;</div>


                <p align="center">
                    <iframe width="784" height="441" src="https://www.youtube.com/embed/p5U4NgVGAwg" frameborder="0"
                            allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                    </iframe>
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>


            </div>

        </div>
    </div>
</div>


<footer class="footer-container text-center">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <p></p>
            </div>
        </div>
    </div>
</footer>

<script>
    document.addEventListener("DOMContentLoaded", function (event) {
        navActivePage();
    });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID 

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

-->
<script type="text/javascript" src="./main.70a66962.js"></script>
</body>

</html>
