<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <meta content="description" name="description">
    <meta name="google" content="notranslate"/>
    <meta content="Mashup templates have been developped by Orson.io team" name="author">

    <!-- Disable tap highlight on IE -->
    <meta name="msapplication-tap-highlight" content="no">

    <link rel="robot-icon" sizes="180x180" href="./assets/robot-icon-180x180.png">
    <link href="./assets/robot-icon-180x180.ico" rel="icon">

    <title>RL - VeilleTech</title>

    <link href="./main.3f6952e4.css" rel="stylesheet">
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
    <nav class="navbar  navbar-fixed-top navbar-default">
        <div class="container">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbar-collapse">
                 <ul class="nav navbar-nav ">
                    <li><a href="./index.html" title="">Home</a></li>
                    <li><a href="./intro.html" title="">Introduction</a></li>
                    <li><a href="./article1.html" title="">Bases théoriques du RL</a></li>
                    <li><a href="./article2.html" title="">Applications du RL</a></li>
                    <li><a href="./article3.html" title="">RL & Finance</a></li>
                    <li><a href="./news.html" title="">Ressources</a></li>
                </ul>


            </div>
        </div>
    </nav>
</header>
<div class="section-container">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
                <div class="card-container">
                    <div class="text-center">
                        <h1 class="h2">RL & Finance</h1>
                    </div>
                    <p style="text-align:center; font-size:120%;">Cet article s'intéresse au potentiel et utilité 
                        d'application de Reinforcement Learning en Finance</p>

                </div>
            </div>



            <div class="col-xs-12">
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Définition du problème</b>
                </p>
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Nous nous intéressons au problème d'optimisation de portefeuille : Nous disposons d'un certain capital que nous
                    pouvons investir pour acquérir certaines actions (dont le prix évolue dans le temps) et nous cherchons à maximiser 
                    notre gain (utilité) sur un certain intervalle de temps. <br>
                    Avant d'explorer ce problème du côté Machine / Reinforcement Learning, fixons d'abord quelques notions pour la suite.
                </p>
                
                <ul style="text-align:justify; font-size:160%; font-weight:400;">
                    
                    <li><b>Un actif</b> est un élément de valeur économique. Exemples d'actifs : liquidités, actions...</li>
                    <div style="height:15px;font-size:10px;">&nbsp;</div>
                    
                    <li><b>Un portefeuille</b> est un ensemble d'actifs financiers multiples, et est déterminé par deux choses : <br>
                        Ses constituants (m actifs dont il est constitué) et un vecteur portefeuille dont la i-ème composante représente 
                        le rapport entre le budget total investi dans l'actif 'i' et le budget total à disposition </li>.
                    <div style="height:15px;font-size:10px;">&nbsp;</div>
                    
                    <li><b>La période de trading est divisée en périodes égales de durée T. Au début de chaque période,
                        notre agent de négociation réattribue le budget disponible entre les actifs, c'est-à-dire que le vecteur portefeuille est
                        mis à jour.<br>
                        Le prix d'un actif fluctue à la hausse et à la baisse au cours d'une période, mais quatre prix importants
                        caractérisent le mouvement global d'une période, à savoir : l'ouverture, le plus haut, le plus bas et les cours 
                        de clôture.</b></li>
                        
                    <li><b> Que veut-on vraiment faire ? </b> Intuitivement, la réponse est : Réaliser le plus de bénéfices possibles.
                    Ceci est en partie vrai, et la métrique qui mesure purement cela est le PnL (Profit and Loss) :
                    C'est le bénéfice net de l'ensemble d'opérations executées, qui peut être positif ou négatif. Une alternative 
                    serait le PnL non réalisé, qui correspond au bénéfice net que l'on obtiendrait en fermant immédiatement toutes 
                    ses positions.<br>
                    Ces deux fonctions de récompense optimisent naïvement le profit. En réalité, un trader peut vouloir minimiser le risque.
                    Une stratégie avec un rendement légèrement inférieur mais une volatilité significativement plus faible est préférable 
                    à une stratégie très volatile mais seulement légèrement plus rentable. L'utilisation du ratio de Sharpe est un moyen 
                    simple de prendre en compte le risque.</li>
                </ul>
                
               
                
                <div style="height:25px;font-size:25px;">&nbsp;</div>
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Approche supervisée et limitations</b>
                </p>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Avant d'examiner le problème du point de vue de l'apprentissage par renforcement, réfléchissons à comment nous
                    pourrions créer une stratégie rentable en utilisant une approche d'apprentissage supervisé. 
                    Ensuite, nous expliquerons ce qui pose problème dans cette approche et pourquoi des techniques 
                    de RL peuvent s'avérer plus efficaces.</p>

                    <div style="height:20px;font-size:20px;">&nbsp;</div>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    L'approche la plus intuitive consiste à penser le problème comme prédiction des prix des actifs : 
                    Si nous pouvons prédire que le marché augmentera, nous pouvons acheter maintenant et vendre une fois que 
                    le marché aura évolué. Ou, de façon équivalente, si nous prévoyons une baisse du marché, 
                    nous pouvons être short (emprunter un actif que nous ne possédons pas) et ensuite acheter une fois 
                    que le marché a évolué.<br>
                    Cela paraît très simple, mais en fait plusieurs questions se posent : </p>
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                    
                        <li>Tout d'abord, quel est le prix que nous prévoyons réellement ? Il n'y a pas un seul prix auquel nous achetons : 
                        Le prix final que nous payons dépend du volume disponible aux différents niveaux du carnet de commandes 
                        et des frais que nous devons payer. <br>
                        Une chose naïve à faire est de prédire le prix moyen, qui est le point médian entre le meilleur cours acheteur
                        et le meilleur cours vendeur. Cependant, il ne s'agit que d'un prix théorique, et non d'un prix auquel nous
                        pouvons exécuter des ordres, et il pourrait différer considérablement du prix réel que nous payons.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>

                        <li>La question suivante est celle de l'échelle de temps. Prévoyons-nous le prix de la prochaine transaction ?
                        Le prix à la seconde suivante ? Une minute ? Une heure ? Jour ? Intuitivement, plus nous voulons prédire l'avenir,
                        plus l'incertitude est grande et plus le problème de prédiction devient difficile.</li>
                    </ul>
                    <div style="height:20px;font-size:20px;">&nbsp;</div>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Pour traduire ces limitations, considérons un exemple concret : Supposons que nous ayons un actif <b>A</b> avec une valeur initiale
                    de départ de P=10,000$. Supposons que nous puissions prédire avec précision que le prix passe de P à 10,050$ dans la minute
                    qui suit. Logiquement, cela signifierait que si nous pouvons réaliser un gain de 50$ en achetant à t=0 et en revendant
                    à t+1, sauf que ...
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                    
                        <li> Un problème peut survenir avec le volume recquis et le prix de vente de <b>A</b> pourrait passer de 10,000$
                            à 10,010$ dès que l'on achète la moitié d'une unité. Donc au final, nous aurons dépensé 10,005$ pour l'achat
                            d'une unité entière de <b>A</b>. </li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>

                        <li>Des coûts de transaction vont certainement survenir (comission du courtier par exemple). Elle sont souvent
                            de l'ordre de 0.3%, ce qui correspondrait ici à 30$</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li>En raison de la vitesse à laquelle le marché évolue, lorsque nous décidons de vendre à un prix donné, 
                            il est possible que toute la quantité ne soit pas vendue à ce prix (comme lorsque nous voulions acheter). 
                            Nous allons donc finir par vendre pour un total de 10.045$ par exemple.<br>
                            Aussi, nous supposons que nous avons des coûts de transaction lors de l'opération de vente aussi, 
                            toujours de l'ordre de 30$.</li>
                    </ul>
                </p>
                    <div style="height:20px;font-size:20px;">&nbsp;</div>
                    
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Au moment du bilan, combien d'argent avons-nous gagné ? -10005 - 30 - 30 + 10045 = -20 ! <b>Au lieu de faire 50$, 
                    nous avons perdu 20$.</b><br>
                    Quelles leçons pouvons-nous tirer de cet exemple ? 
                </p>
                        
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                    
                        <li> Afin de faire des bénifices à partir d'une simple stratégie de prédiction des prix, nous devons prédire
                            des mouvements de prix relativement importants sur de plus longues  périodes de temps, ce qui en soit
                            représente un problème très difficile (plus la période est longue plus l'incertitude est grande). </li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>

                        <li>Un autre problème avec l'apprentissage supervisé : Nous ne récupérons pas / n'apprenons pas une stratégie
                            de trading.<br>
                            Dans l'exemple ci-dessus, nous avons acheté parce que nous avions prédit que le prix augmenterait, 
                            et il a en effet augmenté. Tout s'est déroulé comme prévu. Mais qu'aurions nous fait si le prix avait baissé ? 
                            Vendre ou Garder l'actif et attendre de voir son évolution ultérieure ? 
                            Que se serait-il passé si le prix avait augmenté un peu, puis redescendu ? Que se serait-il passé si nous n'avions pas été certains 
                            de la prédiction, par exemple 55 % en hausse et 45 % en baisse ? Comment choisir le seuil pour passer un ordre? <br>
                            Le modèle supervisé n'apporte aucune information par rapport à toutes ces situations, il ne peut fournir que des probabilités ou des 
                            sorties binaires ici.
                       </li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li>En raison de la vitesse à laquelle le marché évolue, lorsque nous décidons de vendre à un prix donné, 
                            il est possible que toute la quantité ne soit pas vendue à ce prix (comme lorsque nous voulions acheter). 
                            Nous allons donc finir par vendre pour un total de 10.045$ par exemple.<br>
                            Aussi, nous supposons que nous avons des coûts de transaction lors de l'opération de vente aussi, 
                            toujours de l'ordre de 30$.</li>
                    </ul>
                    
                    <div style="height:20px;font-size:20px;">&nbsp;</div>
        
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    A partir de l'analyse de l'exemple précédent et les limitations observées du modèle supervisé, nous comprenons que
                    nous avons besoin de plus qu'un simple modèle de prédiction de prix : Nous avons besoin d'une politique (stratégie)
                    fondée sur des règles qui tient compte des prédictions de prix (qui ne deviennent donc qu'une sous partie du modèle global) 
                    et qui décide ce qu'il faut faire : Passer une commande, ne rien faire, annuler une commande, etc. <br> 
                    Comment en arriver à une telle politique ? Comment optimiser les paramètres des politiques et les seuils 
                    de décision ? La réponse à cela n'est pas évidente, et beaucoup de gens utilisent l'heuristique simple ou l'intuition 
                    humaine. C'est en fait ici qu'intervient toute l'utilité d'un modèle basé sur l'apprentissage par renforcement.
                </p>
            
                <div style="height:25px;font-size:25px;">&nbsp;</div>
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Modélisation par Reinforcement Learning</b>
                </p>
                
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Quand nous abordons un problème de l'angle du supervisé, nous nous intéressons à la variable de sortie et les variables
                    qui vont être utilisées pour prédire cette sortie (prédictors) et nous cherchons le mapping désiré. En RL, il va falloir
                    comprendre à quoi correspond chaque élèment de notre problème pour voir s'il s'adapte bien au framework de RL. <br>
                    Dans le cas présent, voici ce que ça donne :
                </p>
                        
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                    
                        <li><b>Agent : </b>L'agent est notre agent de négociation. Vous pouvez considérer l'agent comme un trader humain 
                            qui ouvre l'interface graphique d'une bourse et prend des décisions de trading basées sur l'état actuel de la bourse 
                            et de son compte.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>

                        <li><b>Environnement : </b>Ce qui est important à noter, c'est qu'il y a beaucoup d'autres agents, 
                            tant humains qu'algorithmes de trading automatiques. Ces agents négocient sur le même marché.<br>
                            Supposons un instant que nous agissons à une échelle minutaire i.e : Nous passons l'action, attendons une minute, 
                            obtenons un nouvel état, prenons une autre mesure, et ainsi de suite. Lorsque nous observons un nouvel état, 
                            ce sera la réponse de l'environnement du marché, ce qui inclut la réponse des autres agents. 
                            Ainsi, du point de vue de notre agent, ces agents font aussi partie de l'environnement. 
                            On ne peut pas les contrôler.<br>
                            Pour simplifier, supposons que nous interagissons avec un environnement complexe unique 
                            qui inclut le comportement de tous les autres agents.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Etat : </b> Dans le cas d'une négociation en bourse, nous n'observons pas l'état complet de 
                            l'environnement. Par exemple, nous ne connaissons pas les autres agents qui se trouvent dans l'environnement, 
                            leur nombre, leur actif, etc. Il s'agit donc d'un processus de décision de Markov partiellement observable.
                            Ce que l'agent observe n'est pas l'état actuel S_t de l'environnement, mais une dérivation de celui-ci.<br>
                            Dans notre cas, l'observation à chaque étape t est simplement l'historique de tous les événements 
                            d'échange reçus jusqu'à l'heure t.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Espace d'actions : </b> Dans l'apprentissage du renforcement, nous faisons une distinction entre 
                            les espaces d'action discrets (finis) et continus (infinis).<br>
                            Dans le problème abordé ici, nous avons deux choix de complexité bien différente. <br>
                            L'approche la plus simple serait d'avoir trois actions : Acheter, conserver et vendre. Cela fonctionne, 
                            mais cela nous limite à passer des ordres au marché et à investir une somme d'argent déterministe à 
                            chaque étape.  Le prochain niveau de complexité serait de laisser notre agent apprendre combien d'argent 
                            investir, par exemple, en fonction de l'incertitude de notre modèle. Cela nous mettrait dans un espace 
                            d'action continu, car nous devons décider à la fois de l'action (discrète) et de la quantité (continue).</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Fonction de récompense : </b> Nous allons choisir ici une des fonctions de mesure de l'utilité de l'agent
                            telles qu'introduites dans la définition du problème.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>

                    </ul>
                    
                    <div style="height:15px;fo cvnt-size:15px;">&nbsp;</div>
        
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Maintenant que nous savons qu'on peut formuler le problème d'optimisation de portefeuille comme un problème
                    d'apprentissage par renforcement peut être utilisé dans le trading, comprenons pourquoi nous voulons l'utiliser 
                    plutôt que des techniques supervisées. <br>
                    <div style="height:10px;fo cvnt-size:10px;">&nbsp;</div>
                        
                    
                    <b>Optimisation de bout en bout</b>  Dans l'approche traditionnelle d'élaboration de stratégies, nous devons
                    franchir plusieurs étapes, avant d'en arriver à la mesure à laquelle nous nous intéressons réellement. 
                    Par exemple, si l'on veut trouver une stratégie avec un retrait maximal de 25 %, il faut former un modèle supervisé, 
                    élaborer une politique fondée sur des règles à l'aide du modèle, tester la politique et optimiser ses 
                    hyperparamètres, et enfin évaluer sa performance par simulation.<br>
                    L'apprentissage par renforcement permet une optimisation de bout en bout et maximise les 
                    récompenses (potentiellement retardées). En ajoutant un terme à la fonction de récompense, nous pouvons par exemple 
                    optimiser sans avoir besoin de passer par des étapes séparées. Par exemple, vous pourriez imaginer donner 
                    une récompense négative importante chaque fois qu'un retrait de plus de 25 % se produit, forçant l'agent 
                    à chercher une autre stratégie. 
                    <<div style="height:10px;fo cvnt-size:10px;">&nbsp;</div>
        
        
                    <b>Stratégies apprises</b>  Au lieu d'avoir besoin de coder manuellement une politique basée sur des règles, 
                    La force du RL est le fait que l'on <b>append</b> directement une politique. Il n'est pas nécessaire pour nous 
                    de spécifier des règles et des seuils tels que "acheter quand vous êtes sûr à plus de 75% que le marché va monter". 
                    C'est ce qui est prévu dans la politique RL, qui optimise la métrique à laquelle nous nous intéressons.
                    Nous supprimons une étape complète du processus d'élaboration de la stratégie ! Et parce que la politique peut
                    être paramétrée par un modèle complexe, tel qu'un réseau de neurones profond, nous pouvons apprendre des politiques
                    qui sont plus complexes et plus puissantes que n'importe quelles règles qu'un trader humain pourrait imaginer.
        
                    Au final, développer des stratégies de trading en utilisant RL ressemble donc à ceci :


      

                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p align="center">
                    <img width="920" height="460" src="./assets/images/RL_finance_pipeline.PNG" class="img-responsive" alt="" align="middle">
                </p>
                <p></p>
                <div style="height:30px;font-size:20px;">&nbsp;</div>




            </div>

        </div>
    </div>
</div>


<footer class="footer-container text-center">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <p></p>
            </div>
        </div>
    </div>
</footer>

<script>
    document.addEventListener("DOMContentLoaded", function (event) {
        navActivePage();
    });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID 

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

-->
<script type="text/javascript" src="./main.70a66962.js"></script>
</body>

</html>
