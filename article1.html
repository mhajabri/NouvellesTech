<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <meta content="description" name="description">
    <meta name="google" content="notranslate"/>
    <meta content="Mashup templates have been developped by Orson.io team" name="author">

    <!-- Disable tap highlight on IE -->
    <meta name="msapplication-tap-highlight" content="no">

    <link rel="robot-icon" sizes="180x180" href="./assets/robot-icon-180x180.png">
    <link href="./assets/robot-icon-180x180.ico" rel="icon">

    <title>Tech Watch</title>

    <link href="./main.3f6952e4.css" rel="stylesheet">
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
    <nav class="navbar  navbar-fixed-top navbar-default">
        <div class="container">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbar-collapse">
                <ul class="nav navbar-nav ">
                    <li><a href="./index.html" title="">Home</a></li>
                    <li><a href="./intro.html" title="">Introduction</a></li>
                    <li><a href="./article1.html" title="">Bases th√©oriques du RL</a></li>
                    <li><a href="./article2.html" title="">Applications du RL</a></li>
                    <li><a href="./article3.html" title="">RL & Finance</a></li>
                    <li><a href="./news.html" title="">Ressources</a></li>
                </ul>

            </div>
        </div>
    </nav>
</header>
<div class="section-container">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
                <div class="card-container">
                    <div class="text-center">
                        <h1 class="h2">Reinforcement Learning : Th√©orie</h1>
                    </div>
                    <p style="text-align:center; font-size:120%;">
                        Cet article pr√©sente les bases th√©oriques du Reinforcement Learning
                    </p>

                </div>
            </div>


            <div class="col-md-8 col-md-offset-2 section-container-spacer">
                <div class="row">
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/rl_article1.png" class="img-responsive" alt="">
                        <p>Saliency Interaction Agent - Environnement </p>
                    </div>
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/rl2_article1.png" class="img-responsive" alt="">
                        <p>R√©seaux de neurones profonds et RL</p>
                    </div>
                </div>
            </div>

            <div class="col-xs-12">
                
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Rappel et exemple concret</b>
                </p>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Rappelons un peu le principe de Reinforcement Learning entrevu dans l'introduction : <br> 
                    Dans l'apprentissage par renforcement, nous avons un <b><i>environnement</i></b> qui repr√©sente le monde ext√©rieur 
                    pour l'agent et un <i><b>agent</i></b> qui effectue des <i><b>actions</i></b>, re√ßoit des observations 
                    de l'environnement qui consiste en une <i><b>r√©compense</i></b> pour son action et des informations 
                    sur son nouvel √©tat. Cette r√©compense informe l'agent du bien ou du mal de l'action entreprise, 
                    et l'observation lui indique quel est son prochain √©tat dans l'environnement. <br>
                    L'agent tente de d√©terminer les actions optimales √† entreprendre ou afin d'accomplir sa t√¢che 
                    de la meilleure fa√ßon possible.
                </p>

                <div style="height:15px;font-size:15px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    <b>Cas concret</b>  Essayons de mod√©liser une situation r√©elle comme un probl√®me de RL. Nous allons aborder le cas
                    d'un b√©b√© qui apprend √† marcher sous la forme d'un probl√®me d'apprentissage par renforcement. <br>
                    Dans ce probl√®me, l'√©tat final id√©al, i.e l'√©tat final qui permet au b√©b√© d'√™tre r√©compens√© est : <b> Le b√©b√© commence
                    √† marcher et arrive √† atteindre une certaine position finale </b>. Dans ce cas l√†, les parents sont tout contents ce qui
                    rend le b√©b√© satisfait et joyeux, nous pouvons dire qu'il re√ßoit donc une r√©compense positive de valeur +1. <br>
                    Le deuxi√®me cas de figure est celui o√π le b√©b√© n'arrive pas √† atteindre la position d√©sir√©e car il tombe en marchant. 
                    Dans ce cas l√†, le b√©b√© se fait mal et pleure : il re√ßoit donc une r√©compens√© n√©gative de valeur -1.<br>
                    Quand le b√©b√© tombera, il essaiera de comprendre ce qui l'a pouss√© √† tomber et il essaiera d'√©viter cette action au futur.
                    De m√™me, s'il ne tombe pas, il essaiera de reproduire sa m√©thode pour les fois prochaines : C'est ainsi que nous humains 
                    apprenons : par essai et erreur (trial and error). L'apprentissage par renforcement est conceptuellement le m√™me, 
                    il s'agit d'une approche computationnelle pour apprendre par l'action.
                </p>
                
                
                <div style="height:25px;font-size:25px;">&nbsp;</div>
                
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Notions et d√©finitions</b>
                </p>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Nous avons √©voqu√© des termes comme 'action' ou encore 'r√©compense'. Maintenant, nous allons mettre en place la 
                    formulation th√©orique d'un probl√®me de RL et d√©finir et d√©finir en passant ce que veulent dire ces termes.
                </p>
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                        
                        <li><b>Agent</b> : Entit√© hypoth√©tique qui accomplit des actions dans un environnement dans le but
                            d'obtenir une certaine r√©compense. </li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>√âtat(s) :</b> Situation actuelle retourn√©e par l'environnement.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Action(a) :</b> Tous les mouvements possibles que l'agent peut prendre, celles-ci peuvent d√©pendre 
                                                de l'√©tat pr√©sent. </li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Environnement (e) :</b> Un sc√©nario auquel l'agent doit faire face.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                       
                        <li><b>R√©compense (R) :</b> Retour de l'environnement permettant d'√©valuer la derni√®re action de l'agent.</li>
                        <div style="height:15px;font-size:10px;">&nbsp;</div>
                        
                        <li><b>Strat√©gie(œÄ) :</b> Il s'agit d'une distribution de probabilit√©s sur les actions √©tant donn√© les √©tats, 
                            c'est-√†-dire comment l'agent choisit ses actions √©tant donn√© qu'il se trouve dans un certain √©tat. 
                            Il pourrait s'agir d'une strat√©gie d√©terministe que nous voulons "apprendre" de l'exp√©rience, 
                            ou une strat√©gie stochastique.</li>
                    </ul>
                </p>
            
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Muni de cela, nous pouvons d√©sormais r√©fl√©chir √† ce que nous voulons maximiser dans un probl√®me de RL, en d'autres termes,
                    nous voulons bien d√©terminer un √©quivalent √† la fonction de co√ªt dans un probl√®me classique de Machine Learning. <br>
                    C'est ici qu'intervient la <b>la fonction de valeur (Value Function)</b> : C'est une fonction qui nous dit √† quel point
                    chaque √©tat et/ou action est bon, c'est-√†-dire √† quel point il est bon d'√™tre dans un √©tat particulier, et √† quel point 
                    il est bon d'entreprendre une action particuli√®re. <br>
                    <div style="height:15px;font-size:10px;">&nbsp;</div>
            
                    Dans un √©tat donn√© S et √† l'instant t, la fonction de valeur informe l'agent de la <b>somme attendue (esp√©rance)
                    des r√©compenses futures pour une strat√©gie adopt√©e ùúã, de mani√®re √† choisir la bonne action qui maximise 
                    cette esp√©rance. Math√©matiquement, cela correspond √† : 
                    
                    <div style="height:15px;font-size:15px;">&nbsp;</div>
                    <p align="center">
                        <img src="./assets/images/value_func.PNG" class="img-responsive" alt="" align="middle">
                    </p>
                    <div style="height:15px;font-size:15px;">&nbsp;</div>

                    ùõæ est un facteur d'actualisation, o√π ùõæ ‚àà [0, 1]. Il informe l'agent de l'importance qu'il doit accorder 
                    aux r√©compenses actuelles et futures : 
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                        <li>ùõæ = 0 correspond √† l'agent est myope : c'est-√†-dire qu'il ne se soucie que de la premi√®re r√©compense. </li>
                        <li>ùõæ = 1 correspond √† l'agent √† une vision √† long terme : c'est-√†-dire qu'il se soucie de toutes les 
                            r√©compenses futures.</li>
                    </ul>
                    
                    
                </p>






            </div>
        </div>
    </div>


    <footer class="footer-container text-center">
        <div class="container">
            <div class="row">
                <div class="col-xs-12">
                    <p></p>
                </div>
            </div>
        </div>
    </footer>

    <script>
        document.addEventListener("DOMContentLoaded", function (event) {
            navActivePage();
        });
    </script>

    <!-- Google Analytics: change UA-XXXXX-X to be your site's ID

    <script>
      (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
          (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
          m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
      })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
      ga('create', 'UA-XXXXX-X', 'auto');
      ga('send', 'pageview');
    </script>

    -->
    <script type="text/javascript" src="./main.70a66962.js"></script>
</body>

</html>
