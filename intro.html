<!DOCTYPE html>
<html lang="en">

    
    
<head>
    <meta charset="UTF-8">
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta content="width=device-width,initial-scale=1" name="viewport">
    <meta content="description" name="description">
    <meta name="google" content="notranslate"/>
    <meta content="Mashup templates have been developped by Orson.io team" name="author">

    <!-- Disable tap highlight on IE -->
    <meta name="msapplication-tap-highlight" content="no">

    <link rel="robot-icon" sizes="180x180" href="./assets/robot-icon-180x180.png">
    <link href="./assets/robot-icon-180x180.ico" rel="icon">

    <title>Tech Watch</title>

    <link href="./main.3f6952e4.css" rel="stylesheet">
</head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
    <nav class="navbar  navbar-fixed-top navbar-default">
        <div class="container">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse"
                    aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbar-collapse">
                <ul class="nav navbar-nav ">
                    <li><a href="./index.html" title="">Home</a></li>
                    <li><a href="./intro.html" title="">Introduction</a></li>
                    <li><a href="./article1.html" title="">GANs</a></li>
                    <li><a href="./article2.html" title="">Beautiful Art vs Terrifying Deepfakes</a></li>
                    <li><a href="./article3.html" title="">What's next ?</a></li>
                    <li><a href="./news.html" title="">News</a></li>
                    <li><a href="./videos.html" title="">Videos</a></li>
                </ul>

            </div>
        </div>
    </nav>
</header>
<div class="section-container">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <img src="./assets/images/work001-01.jpg" class="img-responsive" alt="">
                <div class="card-container">
                    <div class="text-center">
                        <h1 class="h2">Introduction</h1>
                    </div>
                    <p style="text-align:center; font-size:120%;"> Introduction à l'apprentissage par renforcement</p>

                </div>
            </div>


            <div class="col-md-8 col-md-offset-2 section-container-spacer">
                <div class="row">
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/atari_rl.gif" class="img-responsive" alt="">
                        <p>Une des premières applications de RL - Atari</p>
                    </div>
                    <div class="col-xs-12 col-md-6">
                        <img src="./assets/images/alphago.jpeg" class="img-responsive" alt="">
                        <p>Plus récent : AlphaGo de DeepMind</p>
                    </div>
                </div>
            </div>

            <div class="col-xs-12">
                
                <p style="text-align:justify; font-size:200%; font-weight:600;">
                    <b>Branches de Machine Learning</b>
                </p>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    En machine learning, on distingue principalement 3 paradigmes d'apprentissage et qui sont imposés
                    par le type de tâches qu'on cherche à effectuer ainsi que les données à disposition.
                    Ces types d'apprentissage sont :
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                        
                        <li><b>Supervised Learning (apprentissage supervisé) :</b> Dans ce contexte, nous disposons de variables d'entrée (x),
                            une variable de sortie (Y) et nous utilisons un algorithme pour apprendre le mapping entre
                            l'entrée et la sortie. Le terme "supervisé" vient du fait que l'apprentissage d'un algorithme 
                            à partir d'un ensemble de données peut être considéré comme un enseignant qui supervise 
                            le processus d'apprentissage :  Nous connaissons les bonnes réponses, l'algorithme fait des 
                            prédictions itératives sur les données de formation et est corrigé par l'enseignant et cet apprentissage 
                            s'arrête lorsque l'algorithme atteint un niveau de performance acceptable. </li>
                        <div style="height:20px;font-size:10px;">&nbsp;</div>
                        <li><b>Unsupervised Learning (apprentissage non supervisé) :</b> Cas où nous n'avons que des données d'entrée (X) 
                            et aucune variable de sortie correspondante. L'objectif du framework non supervisé est de modéliser 
                            la structure ou la distribution sous-jacente (latente) des données. Dans l'apprentissage non supervisé, 
                            il n'y a pas de bonnes réponses et donc pas d'enseignant : Les algorithmes sont livrés eux-mêmes pour découvrir 
                            et présenter la structure intéressante des données.</li>
                        <div style="height:20px;font-size:10px;">&nbsp;</div>
                        <li><b>Reinforcement Learning (apprentissage par renforcement) :</b> L'apprentissage par renforcement est un aspect du machine
                            learning entièrement différent. Dans le Reinforcement Learning, un agent apprend à se comporter dans un environnement, 
                            en effectuant certaines actions puis en observant les récompenses/résultats obtenus par ces actions.
                            Dès lors, nous avons ce processus action -> conséquence -> action prenant le résultat précédent en compte, etc ...       
                            Cela permet de s'attaquer à des problématiques qui ne peuvent pas être résolues par les deux approches précédentes, par exemple :
                            Comment apprendre à un algorithme d'IA à jouer à un jeu en s'inspirant d'anciennes parties jouées ?</li>
                    </ul>
                </p>

                <div style="height:20px;font-size:20px;">&nbsp;</div>
                <p align="center">
                    <img src="./assets/images/agent_inv.png" class="img-responsive" alt="" align="middle">
                </p>
                <p></p>
            
                <div style="height:30px;font-size:20px;">&nbsp;</div>
            
            
            
                <p style="text-align:justify; font-size:200%; font-weight:6000;">
                    <b>Histoire et débuts du Reinforcement Learning</b>
                </p>
            
                <div style="height:20px;font-size:20px;">&nbsp;</div>

                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Plusieurs disciplines scientifiques et académiques ont contribué à l'apparition de l'apprentissage par renforcement (RL),
                    plus particulièrement, il y en a deux :
                    
                    <ul style="text-align:justify; font-size:160%; font-weight:400;">
                        <li>Contrôle optimal.</li>
                        <li>L'apprentissage des animaux par essais et erreurs.</li>  
                    </ul>
                </p>
            
                    <div style="height:10px;font-size:10px;">&nbsp;</div>
        
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    Les problèmes de contrôle optimal ont été résolus par des méthodes de programmation dynamique (Bellmann 1957) 
                    tandis que l'apprentissage par essais et erreurs a ses racines dans la psychologie, en particulier 
                    le conditionnement classique et le conditionnement instrumental.                                  
                    Le contrôle optimal et le conditionnement instrumental traitent les problèmes de contrôle en boucle fermée. 
                    Cependant, le conditionnement classique traite d'un problème de prédiction uniquement parce que 
                    la réponse de l'animal n'influence pas l'expérience ou, de façon plus générale, n'influence pas l'environnement.
                </p> 
        
                <div style="height:10px;font-size:10px;">&nbsp;</div>            
       
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    L'étude interdisciplinaire de ces deux domaines a donné naissance à une méthode de calcul très influente,
                    appelée méthode d'apprentissage par différence temporelle (TD) (Witten 1977, Sutton et Barto 1981). 
                    A l'origine, l'apprentissage TD était principalement associé à l'apprentissage des animaux 
                    (conditionnement classique), TD a également été utilisé pour résoudre des problèmes de contrôle optimal. 
                    Il convient de noter en particulier les travaux de Watkins (1989) sur le Q-learning, un algorithme de 
                    contrôle basé sur les différences temporelles.
                </p>
        
                <div style="height:10px;font-size:10px;">&nbsp;</div>
        
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp;
                    C'est essentiellement le travail de Klopf (1972, 1975, 1982, 1988), qui a commencé à réunir les méthodes 
                    de la TD avec les théories de l'apprentissage animal. Il a également introduit la différence entre 
                    la rétroaction évaluative et la rétroaction non évaluative, où il associait la rétroaction évaluative 
                    à la rétroaction [[apprentissage supervisé]]. (rétroaction d'un enseignant) et a déclaré à juste titre 
                    que l'environnement ne produit pas d'évaluation. 
                    La rétroaction qui arrive de l'environnement aux capteurs d'une créature ne peut être que non évaluative. 
                    Toute évaluation, dans ce cas, doit être effectuée uniquement en interne par l'animal lui-même. 
                    Comme les animaux ne reçoivent pas de rétroaction évaluative, la RR semble être un exemple d'apprentissage
                    non supervisé. Cependant, cette formulation cache le problème subtil, parfois très troublant, 
                    concernant la façon de définir et poser l'environnement.
                </p>
                
                <div style="height:10px;font-size:10px;">&nbsp;</div>
        
                <p style="text-align:justify; font-size:160%; font-weight:400;">&emsp; 
                    Un compte rendu détaillé de l'histoire de RL se trouve dans le site personnel du considéré Père du RL : Richard Sutton.
                    En voici le [lien](http://www.incompleteideas.net/book/ebook/node12.html).
                </p>

            </div>

        </div>
    </div>
</div>


<footer class="footer-container text-center">
    <div class="container">
        <div class="row">
            <div class="col-xs-12">
                <p></p>
            </div>
        </div>
    </div>
</footer>

<script>
    document.addEventListener("DOMContentLoaded", function (event) {
        navActivePage();
    });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

-->
<script type="text/javascript" src="./main.70a66962.js"></script>
</body>

</html>>
